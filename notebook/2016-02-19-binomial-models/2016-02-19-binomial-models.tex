\documentclass[letterpaper]{article}
\usepackage{polyglossia, fontspec}
\usepackage{amsmath, mathtools}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage[margin = 1.5 in]{geometry}

\title{Binomial Models of Reference Read Counts}
\author{Attila Gulyás-Kovács}
\bibliographystyle{plain}

\begin{document}

\maketitle

\section{Introduction}

\subsection{Goals}

\subsection{The modeled data}

We have \(i=1,...,I\) individuals, \(g=1,...,G\) genes and \(v=1,...,V\)
polymorphic (SNP) sites.  With the notation \(v\in(i,g)\) we will express that site \(v\) is in
gene \(g\) and it is heterozygous in individual \(i\), and we distinguish \(v\)
from \(w\) if \(w\in(j,g)\) and if \(i\neq j\) even if both \(v\) and \(w\) map to
the same site in a reference genome (meaning they are homologous).

We assume only one alternative allele at each site \(v\), and write \(Y_v\) to
denote the read count of the alternative allele at site \(v\).  We also define 
\begin{eqnarray}
\label{eq:Y-ig-def}
%Y_{v} &=& \mathrm{max}(Z_{v}, n_{v} - Z_{v}) \\
Y_{ig} &=&  \{Y_v\}_{v\in(i,g)}, \qquad n_{ig} = \{n_v\}_{v\in(i,g)} \\
\label{eq:Y-def}
Y &=&  [Y_{ig}], \qquad n = [n_{ig}],
%S_{ig} &=& Y_{ig} / n_{ig}.
\end{eqnarray}
where \([Y_{ig}]\) denotes a matrix whose rows are indexed by \(i=1,...,I\)
and columns by \(g=1,...,G\).  Moreover, we have a design matrix \(X =
[X_{ir}], \; r=1,...,R\) whose columns \(x_r\) are explanatory variables
a.k.a.~regressors.  All
proposed inferences in this article will be based on \(Y\) and \(X\).

Much of the previous inferences of the MAE project were based on the statistic
\(S = [S_{ig}]\). The connection between \(S\) and \(Y\) can be drawn by
introducing the ``higher read count'' \(H_{v} = \max(Y_v, n_v-Y_v)\) and
writing \(S_{ig} = \left( \sum_{v\in(i,g)} H_v \right) \times \left(
\sum_{v\in(i,g)} n_v \right)^{-1}\).  The scalar \(S_{ig}\) aggregates the
vectors \(Y_{ig}\) and \(n_{ig}\) and, as we will see, the information lost in
that aggregation has an impact on all statistical analysis based on the models
below.

\subsection{Definition of bi and monoallelic expression}

A prerequisite of the following definition is the assumption that \(Y_v\) is
binomially distributed with parameters \(n_v\) (the total read counts) and
\(q_{ig}\).  Thus, for all sites \(v\in(i,g)\) the expected fraction
\(\mathrm{E}[Y_v]/n_v = q_{ig}\).
We regard the expected proportion \(q_{ig}\) the single direct determinant of
allelic exclusion based on which we can define bi and monoallelic expression
as follows.

Informally speaking, we define the biallelic case such that the two alleles
are expressed equally, so \(q_{ig}=1/2\), and the monoallelic case with
\(q_{ig}\) close to either 1 or 0 depending on whether the reference or the
alternative allele is excluded, respectively.  To express our indifference
about that last point we introduce \(p_{ig} = \max(q_{ig},
1-q_{ig}) \), which implies that \(1/2\le p_{ig}\le 1\).

For the formal definition we introduce variable \(\theta_{ig}\)
indicating the biallelic and monoallelic case (\(\theta_{ig}=0\) and \(\theta_{ig}=1\),
respectively).  We also fix parameters \(p_0,p_1\) by setting \(p_0=1/2\) and
\(p_1=0.9\), say.  We then define
allelic exclusion with the general equation \(p_{ig} = p_{\theta_{ig}}\) or,
equivalently, with
\begin{equation}
\begin{array}{rcccc}
\label{eq:hypotheses}
\text{allelic exclusion} & & \text{indicator} & & \text{expected
proportion} \\
\hline
\text{biallelic exp.~of } (i,g) & \Leftrightarrow & \theta_{ig}=0 &
\Leftrightarrow & p_{ig}=p_0 \\
\text{monoallelic exp.~of } (i,g) & \Leftrightarrow & \theta_{ig}=1 &
\Leftrightarrow & p_{ig}=p_1
\end{array}
\end{equation}

A few things deserve mentioning in the context of Eq.~\ref{eq:hypotheses}.
\begin{enumerate}
\item By indexing \(\theta\) and \(p\) using both \(i\) and \(g\) we allow variation
in allelic exclusion not only across genes but also across individuals,
\item we define monoallelic expression by a theoretical expectation based on a
simple parametric model rather than referring to some previous gold standard
data set of \((i,g)\) pairs that have been classified as either bi or monoallelically
expressing,
\item the choice of \(p_0=1/2\) leaves little room for debate but that of
\(p_1\) is quite arbitrary, and \(p_1\) will in general influence all outcomes of
statistical inference; so the results must be interpreted in light of the
definition,
\item using only two classes (bi and monoallelic expression) means only two possible
values of \(p_{ig}\) so we cannot account for  
relatively subtle differences among individuals and/or genes by fine-tuning
\(p_{ig}\); this constraints the way we can model dependence on age across all
individuals for a given gene, or dependence on distance from previously
identified imprinted genes across all genes for a given individual.
\end{enumerate}

\subsection{Latent and observable variables}

Our preference of \(p_{ig}\) to \(q_{ig}\) motivates the introduction
of
\begin{equation}
\label{eq:Z-def}
Z_v =
\begin{cases}
Y_v & \text{if } p_{ig} \ge 1/2 \\
n_v-Y_v & \text{otherwise}.
\end{cases}
\end{equation}
where \(v\in(i,g)\).
Then \(Z_v\sim\mathrm{Binom}(p_{ig},n_v)\) if and only if
\(Y_v\sim\mathrm{Binom}(q_{ig},n_v)\).  Using \(Z_v\) facilitates
expressing models in the most direct manner (Section~\ref{sec:models}).  However, \(Z_v\) is a latent
(unobserved) variable because we are uncertain about \(p_{ig}\).  For
this reason, statistical inference will require using likelihood functions
based on \(Y_v\) (Section~\ref{sec:likelihood} and~\ref{sec:inference}).


\section{Models}
\label{sec:models}

\newcounter{model}
\renewcommand{\thesubsection}{M\arabic{model}}

\stepcounter{model}\subsection{}
\label{sec:model-basic}

\section{Likelihood functions}
\label{sec:likelihood}

\subsection{Sampling distribution for read counts \(y_{ig}\)}

\begin{equation}
f_v(y_v | n_v, p_a) = \binom{n_v}{y_v} p_a^{y_v} (1-p_a)^{n_v-y_v}
\end{equation}

The p.m.f.~for \(y_{ig}\)
\begin{equation}
f_{ig}(y_{ig} | n_{ig}, p_a, \kappa) = \prod_{v\in(i,g)}
\left[
\kappa f_v(y_v | n_v, p_a) +
(1-\kappa) f_v(y_v | n_v, 1-p_a)
\right]
\end{equation}

\subsection{Marginal likelihood for \(\pi\)}
\label{sec:marginal-likelihood-pi}

model M.I.1; the marginal likelihood \(L(\pi;y,n,p,\kappa)\equiv f(y|n,p,\kappa,\pi)\) for \(\pi\) equals
\begin{equation}
L(\pi) = \prod_{i,g}
\left[
(1-\pi) f_{ig}(y_{ig} | n_{ig}, p_0, \kappa)
+
\pi f_{ig}(y_{ig} | n_{ig}, p_1, \kappa)
\right]
\end{equation}

model M.I.2; the marginal likelihood \(L(\pi;y,n,p,\kappa,\nu)\equiv
f(y|n,p,\kappa,\nu,\pi)\) for \(\pi\) is given by
\begin{eqnarray}
L(\pi) &=& B^{-1} \prod_{g} \int_{0}^{1} \mu^{\pi\nu} (1-\mu)^{(1-\pi)\nu}
\prod_{i}
u_{ig}(\mu) \, \mathrm{d}\mu
\\
u_{ig}(\mu) &=&
(1-\mu) f_{ig}(y_{ig} | n_{ig}, p_0, \kappa)
+
\mu f_{ig}(y_{ig} | n_{ig}, p_1, \kappa)
\end{eqnarray}
where \(B\) is the beta function evaluated at \((\pi\nu, (1-\pi)\nu)\).

\section{Inference}
\label{sec:inference}

Given the models in Section~\ref{sec:models} and their parameters, the goals
of the study can be framed in the following statistical inference tasks:
\begin{enumerate}
\item assess dependence on explanatory variables via two tightly linked tasks:
\begin{itemize}
\item \emph{select the model}\footnote{When several models are nearly equally
good, it is preferred to avoid selecting only one of them and discard the
rest.  In that case Bayesian model averaging provides a normative solution. } that best fits both the data and some prior information
such as definitions or theoretical considerations
\item \emph{estimate} regression parameters \(\beta_h\) (Eq.???)
\end{itemize}
\item assess the fraction of monoallelically expressed genes by finding an
\emph{estimate} \(\hat{\pi}_1\) for \(\pi_1\)
\item call novel monoallelically expressed genes: depending on the selected
model \emph{classify} each \((i,g)\) or \(g\) by hypothesis testing
(Eq.~\ref{eq:hypotheses})
\end{enumerate}

\begin{table}[t]
\center
\begin{tabular}{c||c|c|c|}
\hline
strategy & \multicolumn{2}{|c|}{conditional (sequential)} & joint \\
\hline
inference task(s) & \parbox{3.5 cm}{\center model selection,\\parameter estimation} & classification & all \\
\hline
\parbox{2 cm}{\center required\\prior info} & training
set & known model & basic assumptions \\
\hline
\end{tabular}
\caption{Two basic strategies for carrying out inference tasks relevant to the project.}
\label{tab:inference-strategies}
\end{table}

Depending on what prior information we wish to take advantage of, we may
choose between two major strategies, summarized by
Table~\ref{tab:inference-strategies}.  The conditional strategy requires prior
information beyond the basic assumptions, where the latter correspond to the
constraints of the most general model we consider
(??? in Section~\ref{sec:models}).

One such piece of prior information is a \emph{training set} of \((i,g)\)
pairs (or of genes \(g\)) that are labeled either as mono or biallelically
expressing.  Given the training set the best model can be selected and most
parameters (like \(\beta\)) can be estimated.  Parameter \(\pi_1\), however,
is special in the sense that it can only be estimated from the genome-wide
test data (or its addressable subset).

The conditional strategy is also sequential in that in the first step model selection and
the estimation of \(\beta\) must be achieved, then based on that the
estimation of \(\pi_1\) together with classification.

In principle it is possible to evade the discomforting uncertainty that may
surround prior information by ignoring those completely.  This, however,
requires a joint inference strategy that is both challenging to implement and
validate and may lead to high errors in all three tasks depending on how
valuable the discarded prior information are.

\subsection{Classification}

\section{Appendix}
\label{sec:appendix}

If we want to base inference on the scalar \(S_{ig}\) instead of the vector
\(Y_{ig}\), we need to derive likelihood functions for \(S_{ig}\) using
Eq.???.
Let \(\mathcal{S} = \{(i,g) : n_{ig} s_{ig} = y_{ig}\}\), that is the set of
all \((i,g)\) pairs leading to the observed \(s_{ig}\).  Then the likelihood
functions \(h_{ig}\) and \(h'_{ig}\) for \(S_{ig}\) can be expressed in terms
of \(\{f_{ig}\}_{(i,g)\in\mathcal{S}}\):
\begin{eqnarray}
\label{eq:S-pmf-given-n}
h_{ig}(s_{ig} | n_{ig}, p_h) &=& \sum_{(i,g)\in\mathcal{S}} f_{ig}(y_{ig} | n_{ig}, p_h)
\\
\label{eq:S-pmf-given-dist-of-n}
h'_{ig}(s_{ig} | p_h) &=& \sum_{(i,g)\in\mathcal{S}} f_{ig}(y_{ig} | n_{ig},
p_h) \, q_{ig}(n_{ig}|p_h).
\end{eqnarray}
The difference between \(h_{ig}\) and \(h'_{ig}\) is whether or not we
condition the distribution of \(S_{ig}\) on the observed \(n_{ig}\).  If we
don't take advantage of the observations on \(n_{ig}\) (Eq.~\ref{eq:S-pmf-given-dist-of-n}), we
must then treat it as a random variable and specify a distribution for it, say
\(q_{ig}\). In either case we need \emph{some} kind of information or
assumption on
\(n_{ig}\).  This holds regardless we want to use \(h_{ig}\) (or \(h'_{ig}\))
in simulations, in parameter estimation or in classification with error
control.


\end{document}

\section{Basic model}

In my understanding, in Andy's general model
\(\{Y_{ig}\}_{ig}\) are independent random variables and
\begin{equation}
Y_{ig} \sim \mathrm{Binom}(q_h \text{ or } 1 - q_h, n_{ig}) \text{ under }
\mathcal{H}_h, \; h=0,1
\end{equation}

Let \(p_h = \mathrm{max}(q_h, 1-q_h)\).  In Andy's specific model \(p_0 =
1/2\) and \(p_1=9/10\).  To specify the model more completely, suppose \(p_h =
q_h\) with \(1/2\) probability \emph{a priori}.  Then for each \((i,g)\) the
probability mass function of \(Y_{ig}\)'s sampling distribution is
\begin{equation}
f(y|p_h, n_{ig}) = \frac{1}{2} \frac{n_{ig}!}{y! (n_{ig}-y)!} \left[ p_h^{y}
(1-p)^{n_{ig}-y} + p^{n_{ig}-y} (1-p)^{y} \right].
\end{equation}

Note that for homozygous \((i,g)\) pairs \(f(y=n_{ig}|p_h,n_{ig})=1\) for \(h=0,1\)
because all reads must surely come from a single variant regardless of allelic
exclusion.


For the observation \(Y_{ig}=y_{ig}\) the \(p\)-value is 
\begin{equation}
\sum_{y=y_{ig}}^{n_{ig}} f(y|p_0,n_{ig}).
\end{equation}

Set classification threshold \(n_{ig} t\) for any \(Y_{ig}\).  For instance,
\(t=0.9\) means that we classify those pairs \((i,g)\) for which at least
\(9/10\) of the reads come from the reference allele.  Let
\(\pi_0\) and \(\pi_1\) be the fraction of \((i,g)\) pairs when
\((i,g)\in\mathcal{H}_0\) and when
\((i,g)\in\mathcal{H}_1\), respectively.  Note that \(\pi_0+\pi_1=1\).

The expected number of \((i,g)\) pairs called monoallelic is then
\begin{equation}
\sum_{i,g} \pi_0 \overbrace{ \sum_{y=t}^{n_{ig}} f(y|p_0,n_{ig})}^{\text{false
positive rate}} + \pi_1
\overbrace{ \sum_{y=t}^{n_{ig}} f(y|p_1,n_{ig})}^{\text{true positive rate}}.
\end{equation}

So, given \(t\), there are two ways to learn about the expected number of positives
Define \(S_{ig}=n_{ig}^{-1} \mathrm{max}(Y_{ig}, n_{ig} - Y_{ig})\).  Then we
have
\begin{equation}
\label{eq:pmf-s-pmf-y}
f_s(s|p_h,n_{ig}) = f(y|p_h,n_{ig})
\end{equation}
as a consequence of the definition of \(p_h\), so it doesn't matter if we use
\(S_{ig}\) or \(Y_{ig}\) for testing \(H_h\) or inferring \(p_h\) as long as
we use the information \(n_{ig}\).  If may remove \(n_{ig}\) from
Eq.~\ref{eq:pmf-s-pmf-y} if we take it as a random quantity, specify a
distribution for it, and marginalize \(f_s\).  But then
\begin{equation}
\label{eq:pmf-smarginal-pmf-y}
f_s(s|p_h) \neq f(y|p_h,n_{ig})
\end{equation}
because we lost the information in the observed total number of reads
\(n_{ig}\).  This information loss prevents \(S_{ig}\) from being a sufficient
statistic.

\section{Likelihood function}
\label{sec:likelihood}

Likelihood functions\footnote{The notion of probability mass/density function
\(f(y|p)\) of statistic \(y\) given parameters \(p\) is so closely
related to the likelihood function \(L(p; y)\) of \(p\) given \(y\)
that the two are often used interchangeably in the literature setting
mathematical rigour aside.  Here I follow this tradition and denote both kinds
of function with \(f\).  } play indispensable role in all forms of inference
relevant to this study: model selection, parameter estimation and
classification.  This section derives the likelihood function \(f\) for the
basic model~\ref{sec:model-basic} based on the observation \(n\) and that
\(Y=y\).  The analogous functions based on \(S=s\) are presented in the
Appendix (Section~\ref{sec:appendix}).  Extensions of \(f\) to more complex models
\ref{sec:model-beta}-\ref{sec:model-prior-evidence} will be presented
in a subsequent report.

By exploiting independencies, \(f\) can be derived piece-wise
based on the set of functions \(\{f_{ig}\}_{ig}\), where each \(f_{ig}\) in
turn is derived from \(\{f_v\}_{v\in(i,g)}\):
%Classification of some \((i,g)\) pair (or \(g\) in regression models) will
%require only \(f_{ig}\) (or \(f_g\)) because of the independencies of the
%model at hand.
\begin{eqnarray}
\label{eq:f_v}
f_v(y_v | n_v, p_h) &=& \frac{1}{2} \binom{n_v}{y} \left[
p_h^{y_v} (1 - p_h)^{n_v - y} + 
p_h^{n_v - y_v} (1 - p_h)^y \right] \\
\label{eq:f_ig}
f_{ig}(y_{ig} | n_{ig}, p_h) &=& \prod_{v\in(i,g)} f_v(y_v | n_v, p_h) \\
\label{eq:f}
f(y | n, p_0, p_1, \pi_1) &=& \prod_{i,g} \left[
f_{ig}(y_{ig} | n_{ig}, p_1) \pi_1 +
f_{ig}(y_{ig} | n_{ig}, p_0) (1-\pi_1)
\right]
\end{eqnarray}

Eq.~\ref{eq:f_v} follows from the fact that \(Y_v\) is binomially distributed
with proportion parameter either \(p_h\) or \(1-p_h\), and we assume that
these alternative cases are equally likely.  Eq.~\ref{eq:f_ig} expresses
independence of read counts at different polymorphic sites within gene \(g\),
whereas Eq.~\ref{eq:f} follows from the independence of read counts in
model~\ref{sec:model-basic} both across genes and individuals and from the
\emph{a priori} probability \(\pi_1\) of gene \(g\) being monoallelically expressed
in individual \(i\).

\section{Models}
\label{sec:models}

The following models are sequentially nested in each other.  Therefore it is
sufficient to fully describe only the first model in the sequence and only
specify the direction of generalization for the second, third,...~model.
Conversely, the sequence of models can be given in the opposite direction by
specifying the sequence of constraints to obtain from a given model a
more specific model.

\newcounter{model}
\renewcommand{\thesubsection}{M\arabic{model}}

\stepcounter{model}\subsection{}
\label{sec:model-basic}

Model~\ref{sec:model-basic} is the most basic among all models.  It expresses the following \emph{assumptions}:
\begin{enumerate}
\item\label{enum:binom} at any polymorphic site \(v\), \(Z_v\) is binomial with parameters \(n_v,p_h\); the latter being the
expected fraction of \(Z_v/n_v\) when
\(v\in(i,g)\) and \((i,g)\in\mathcal{H}_h\) (Eq.~\ref{eq:hypotheses})
\item\label{enum:fixed-p} \(p_h\) is fixed for all sites
\item\label{enum:shared-p_h} all individuals and all biallelically (or monoallelically) expressed genes share
the same \(p_0\) (or \(p_1\)) regardless of explanatory variables
\item\label{enum:prior} the prior probability \(\pi_1\) of gene \(g\) being monoallelically expressed
in individual \(i\) is the same for all \((i,g)\) pairs regardless of any
prior information, e.g.~known cis-eQTLs in \((i,g)\)
\end{enumerate}

\begin{eqnarray}
\label{eq:hypothesis-prior}
P\left( (i,g) \in \mathcal{H}_h \right) &=& \pi_h \quad \text{\emph{a priori}} \\
\label{eq:fixed-hypothesis-prior}
\pi_h && \text{fixed} \\
\label{eq:binom}
Z_v &\sim& \mathrm{Binom}(p_h, n_v) \quad v\in(i,g), \; (i,g)\in \mathcal{H}_h \\
\label{eq:fixed_pi-p}
p_h && \mathrm{fixed}
\end{eqnarray}

\stepcounter{model}\subsection{}
\label{sec:model-beta}

Relaxing assumption~\ref{enum:fixed-p} means expressing uncertainty about \(p_h\), which
can enhance the robustness of the model.
\begin{eqnarray}
Z_v &\sim& \mathrm{Binom}(p'_h, n_v) \quad v\in(i,g), \; (i,g)\in \mathcal{H}_h \\
\label{eq:beta}
p'_h &\sim& \mathrm{Beta}(\mu_h, \nu_h)
\end{eqnarray}
To obtain model~\ref{sec:model-basic} by constraining~\ref{sec:model-beta}, take \(\mu_h=p_h\) from
Eq.~\ref{eq:binom}-\ref{eq:fixed_pi-p} and let \(\nu_h \rightarrow \infty\).

\stepcounter{model}\subsection{}
\label{sec:model-regression}

Relaxing assumption~\ref{enum:shared-p_h} allows the explanatory
variables \(x_i\) to influence the expected fraction \(Z_v/n_v\).
\begin{eqnarray}
p'_h &\sim& \mathrm{Beta}(\mu'_{hi}, \nu_h) \\
\label{eq:glm}
\mathrm{link\_function}(\mu'_{hi}) &=& x_i \beta_h
\end{eqnarray}
Choosing the best link function is a matter of mechanistic considerations and
model selection comparing several alternative link functions.  To obtain
model~\ref{sec:model-beta} by constraining~\ref{sec:model-regression}, take
\(\beta_{h,0}=\mathrm{link\_function}(\mu'_{hi})\) from Eq.~\ref{eq:beta} and
set \(\beta_{h,1}=...=\beta_{h,p-1}=0\).

\stepcounter{model}\subsection{}
\label{sec:model-prior-evidence}

Prior to observing the RNA-seq data there is evidence \(\mathrm{Ev}_{ig}\)
for/against \((i,g)\in \mathcal{H}_h\) such as
\begin{itemize}
\item distance of \(g\) from known imprinted genes
\item cis-eQTLs of \((i,g)\)
\item confidence in calling \((i,g)\) heterozygous at \(v\)
\end{itemize}

\begin{eqnarray}
P\left( (i,g) \in \mathcal{H}_h\, |\, \mathrm{Ev}_{ig} \right) &=&
\pi'_h(\mathrm{Ev}_{ig}),
\end{eqnarray}
where \(\pi'_h\) is some function of the evidence \(\mathrm{Ev}_{ig}\).  For
instance, \(\mathrm{Ev}_{ig}\) may be gene \(g\)'s distance \(d(g)\) from the
nearest imprinted gene, and \(\pi'_h(\mathrm{Ev}_{ig}) = \gamma +
\mathrm{exp}(- d(g) / \tau) \), where \(\tau\) is a length constant measured
in bases.  To obtain model~\ref{sec:model-regression}
from~\ref{sec:model-prior-evidence}, let \(pi'_h\) be
constant  by setting \(\pi'_h = \pi_h\) from
Eq.~\ref{eq:hypothesis-prior}-\ref{eq:fixed-hypothesis-prior} regardless of
the evidence.

\renewcommand{\thesubsection}{\arabic{section}.\arabic{subsection}}
