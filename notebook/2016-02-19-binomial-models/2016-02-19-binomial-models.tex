\documentclass[letterpaper]{article}
\usepackage{polyglossia, fontspec}
\usepackage{amsmath, mathtools}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage[margin = 1.5 in]{geometry}

\title{Binomial Models of Reference Read Counts}
\author{Attila Gulyás-Kovács}
\bibliographystyle{plain}

\begin{document}

\maketitle

\section{Preliminaries}

We have \(i=1,...,I\) individuals, \(g=1,...,G\) genes and \(v=1,...,V\)
polymorphic (SNP) sites that occur at least one \((i,g)\) pair in heterozygous form.  For
each \((i,g)\) we test hypothesis \(\mathcal{H}_0\) against \(\mathcal{H}_1\):
\begin{equation}
\label{eq:hypotheses}
(i,g) \in \mathcal{H}_h :
\begin{cases}
(i,g) \text{ biallelically expressed} & \text{if } h=0 \\
(i,g) \text{ monoallelically expressed} & \text{if } h=1
\end{cases}
\end{equation}

Assuming only one alternative allele at each \(v\), let \(A_v\) denote the
read count for the alternative allele and \(n_v\) the count of all reads.
Thus, the read count for the reference allele is \(n_v-A_v\). In the context
of all models to follow, we will consider \(n_v\) as observed and fixed
parameter while \(A_v\) as an observed \emph{random variable} with unknown
mean (expected value) \(\mathrm{E}[A_v]\).  

We define
\begin{equation}
\label{eq:Z-def}
Z_v =
\begin{cases}
A_v & \text{if } \mathrm{E}[A_v] \ge n_v-\mathrm{E}[A_v] \\
n_v-A_v & \text{otherwise}.
\end{cases}
\end{equation}
In words, \(Z_v\) is the read count for the allele with the higher expected
read count.

Since the mean counts in Eq.~\ref{eq:Z-def} are unknown, \(Z_v\) is a
\emph{latent (unobserved) variable} in the sense that we don't know for sure whether \(Z_v\) corresponds
to the reference or the alternative allele.  But it will be much more
straight-forward to express all models in Section~\ref{sec:models} using the
\emph{expected fraction} \(p_v=\mathrm{E}[Z_v/n_v]\) instead of the expected fraction
of \(A_v\) in \(n_v\).

Thus \(Z_v\) is latent; but any statistical analysis (parameter inference and hypothesis
testing/classification) must be based on \emph{observed variables}.  To that
end we could use \(A_v\); but to be consistent with the previous work of the
MAE project, we define 
\begin{eqnarray}
\label{eq:Y-def}
Y_{v} &=& \mathrm{max}(Z_{v}, n_{v} - Z_{v}) \\
Y_{ig} &=&  \{Y_v\}_{v\in(i,g)}, \qquad n_{ig} = \{n_v\}_{v\in(i,g)} \\
Y &=&  \{Y_{ig}\}_{ig}, \qquad n = \{n_{ig}\}_{ig}.
%S_{ig} &=& Y_{ig} / n_{ig}.
\end{eqnarray}
The random variable \(Y_v\)\footnote{The symbol \(H\) was used previously in
the MAE project but conventions in statistics and information theory as well
as other considerations motivated me to replace it with \(Y\).} is the
\emph{higher read count} at polymorphic site \(v\).  The notation
\(v\in(i,g)\) means all heterozygous sites \(v\) in individual \(i\) and gene
\(g\).

Much of
the previous analysis of the MAE project was based on \(S = \{S_{ig}\}_{ig}\),
where
\begin{equation}
S_{ig} = \frac{\sum_{v\in(i,g)} Y_v}{ \sum_{v\in(i,g)} n_v} =
\frac{||Y_{ig}||_1}{||n_{ig}||_1}.
\end{equation}
The scalar \(S_{ig}\) aggregates the vectors \(Y_{ig}\) and \(n_{ig}\) and,
as we will see, the information lost in that aggregation has an impact on all
statistical analysis based on the models below.

\section{Models}
\label{sec:models}

The following models are sequentially nested in each other.  Therefore it is
sufficient to fully describe only the first model in the sequence and only
specify the direction of generalization for the second, third,...~model.
Conversely, the sequence of models can be given in the opposite direction by
specifying the sequence of constraints to obtain from a given model a
more specific model.

\newcounter{model}
\renewcommand{\thesubsection}{M\arabic{model}}

\stepcounter{model}\subsection{}
\label{sec:model-basic}

Model~\ref{sec:model-basic} is the most basic among all models.  It expresses the following \emph{assumptions}:
\begin{enumerate}
\item\label{enum:binom} at any polymorphic site \(v\), \(Z_v\) is binomial with parameters \(n_v,p_h\); the latter being the
expected fraction of \(Z_v/n_v\) when
\(v\in(i,g)\) and \((i,g)\in\mathcal{H}_h\) (Eq.~\ref{eq:hypotheses})
\item\label{enum:fixed-p} \(p_h\) is fixed for all sites
\item\label{enum:shared-p_h} all individuals and all biallelically (or monoallelically) expressed genes share
the same \(p_0\) (or \(p_1\)) regardless of explanatory variables
\item\label{enum:prior} the prior probability \(\pi_1\) of gene \(g\) being monoallelically expressed
in individual \(i\) is the same for all \((i,g)\) pairs regardless of any
prior information, e.g.~known cis-eQTLs in \((i,g)\)
\end{enumerate}

\begin{eqnarray}
\label{eq:hypothesis-prior}
P\left( (i,g) \in \mathcal{H}_h \right) &=& \pi_h \quad \text{\emph{a priori}} \\
\label{eq:fixed-hypothesis-prior}
\pi_h && \text{fixed} \\
\label{eq:binom}
Z_v &\sim& \mathrm{Binom}(p_h, n_v) \quad v\in(i,g), \; (i,g)\in \mathcal{H}_h \\
\label{eq:fixed_pi-p}
p_h && \mathrm{fixed}
\end{eqnarray}

\stepcounter{model}\subsection{}
\label{sec:model-beta}

Relaxing assumption~\ref{enum:fixed-p} means expressing uncertainty about \(p_h\), which
can enhance the robustness of the model.
\begin{eqnarray}
Z_v &\sim& \mathrm{Binom}(p'_h, n_v) \quad v\in(i,g), \; (i,g)\in \mathcal{H}_h \\
\label{eq:beta}
p'_h &\sim& \mathrm{Beta}(\mu_h, \nu_h)
\end{eqnarray}
To obtain model~\ref{sec:model-basic} by constraining~\ref{sec:model-beta}, take \(\mu_h=p_h\) from
Eq.~\ref{eq:binom}-\ref{eq:fixed_pi-p} and let \(\nu_h \rightarrow \infty\).

\stepcounter{model}\subsection{}
\label{sec:model-regression}

Relaxing assumption~\ref{enum:shared-p_h} allows the explanatory
variables \(x_i\) to influence the expected fraction \(Z_v/n_v\).
\begin{eqnarray}
p'_h &\sim& \mathrm{Beta}(\mu'_{hi}, \nu_h) \\
\mathrm{link\_function}(\mu'_{hi}) &=& x_i \beta_h
\end{eqnarray}
Choosing the best link function is a matter of mechanistic considerations and
model selection comparing several alternative link functions.  To obtain
model~\ref{sec:model-beta} by constraining~\ref{sec:model-regression}, take
\(\beta_{h,0}=\mathrm{link\_function}(\mu'_{hi})\) from Eq.~\ref{eq:beta} and
set \(\beta_{h,1}=...=\beta_{h,p-1}=0\).

\stepcounter{model}\subsection{}
\label{sec:model-prior-evidence}

Prior to observing the RNA-seq data there is evidence \(\mathrm{Ev}_{ig}\)
for/against \((i,g)\in \mathcal{H}_h\) such as
\begin{itemize}
\item distance of \(g\) from known imprinted genes
\item cis-eQTLs of \((i,g)\)
\item confidence in calling \((i,g)\) heterozygous at \(v\)
\end{itemize}

\begin{eqnarray}
P\left( (i,g) \in \mathcal{H}_h\, |\, \mathrm{Ev}_{ig} \right) &=&
\pi'_h(\mathrm{Ev}_{ig}),
\end{eqnarray}
where \(\pi'_h\) is some function of the evidence \(\mathrm{Ev}_{ig}\).  For
instance, \(\mathrm{Ev}_{ig}\) may be gene \(g\)'s distance \(d(g)\) from the
nearest imprinted gene, and \(\pi'_h(\mathrm{Ev}_{ig}) = \gamma +
\mathrm{exp}(- d(g) / \tau) \), where \(\tau\) is a length constant measured
in bases.  To obtain model~\ref{sec:model-regression}
from~\ref{sec:model-prior-evidence}, let \(pi'_h\) be
constant  by setting \(\pi'_h = \pi_h\) from
Eq.~\ref{eq:hypothesis-prior}-\ref{eq:fixed-hypothesis-prior} regardless of
the evidence.

\renewcommand{\thesubsection}{\arabic{section}.\arabic{subsection}}
\section{Likelihood function}

Likelihood functions\footnote{The notion of probability mass/density function
\(f(y|\theta)\) of statistic \(y\) given parameters \(\theta\) is so closely
related to the likelihood function \(L(\theta; y)\) of \(\theta\) given \(y\)
that the two are often used interchangeably in the literature setting
mathematical rigour aside.  Here I follow this tradition and denote both kinds
of function with \(f\).  } play indispensable role in all forms of inference
relevant to this study: model selection, parameter estimation and
classification.  This section derives the likelihood function \(f\) for the
basic model~\ref{sec:model-basic} based on the observation \(n\) and that
\(Y=y\).  The analogous functions based on \(S=s\) are presented in the
Appendix (Section~\ref{sec:appendix}).  Extensions of \(f\) to more complex models
\ref{sec:model-beta}-\ref{sec:model-prior-evidence} will be presented
in a subsequent report.

By exploiting independencies, \(f\) can be derived piece-wise
based on the set of functions \(\{f_{ig}\}_{ig}\), where each \(f_{ig}\) in
turn is derived from \(\{f_v\}_{v\in(i,g)}\):
%Classification of some \((i,g)\) pair (or \(g\) in regression models) will
%require only \(f_{ig}\) (or \(f_g\)) because of the independencies of the
%model at hand.
\begin{eqnarray}
\label{eq:f_v}
f_v(y_v | n_v, p_h) &=& \frac{1}{2} \binom{n_v}{y} \left[
p_h^{y_v} (1 - p_h)^{n_v - y} + 
p_h^{n_v - y_v} (1 - p_h)^y \right] \\
\label{eq:f_ig}
f_{ig}(y_{ig} | n_{ig}, p_h) &=& \prod_{v\in(i,g)} f_v(y_v | n_v, p_h) \\
\label{eq:f}
f(y | n, p_0, p_1, \pi_1) &=& \prod_{i,g} \left[
f_{ig}(y_{ig} | n_{ig}, p_1) \pi_1 +
f_{ig}(y_{ig} | n_{ig}, p_0) (1-\pi_1)
\right]
\end{eqnarray}

Eq.~\ref{eq:f_v} follows from the fact that \(Y_v\) is binomially distributed
with proportion parameter either \(p_h\) or \(1-p_h\), and we assume that
these alternative cases are equally likely.  Eq.~\ref{eq:f_ig} expresses
independence of read counts at different polymorphic sites within gene \(g\),
whereas Eq.~\ref{eq:f} follows from the independence of read counts in
model~\ref{sec:model-basic} both across genes and individuals and from the
\emph{a priori} probability \(\pi_1\) of gene \(g\) being monoallelically expressed
in individual \(i\).

\section{Inference}

\begin{tabular}{c|c|c|c|}
& \multicolumn{2}{c}{conditional} & joint \\
task & all & \parbox{5 cm}{model selection\\parameter estimation} & classification \\
& & & \\
& & & \\
\end{tabular}

\subsection{Parameter estimation}

\subsection{Classification}

\section{Appendix}
\label{sec:appendix}

If we want to base inference on the scalar \(S_{ig}\) instead of the vector
\(Y_{ig}\), we need to derive likelihood functions for \(S_{ig}\) using
Eq.~\ref{eq:f_ig}.
Let \(\mathcal{S} = \{(i,g) : n_{ig} s_{ig} = y_{ig}\}\), that is the set of
all \((i,g)\) pairs leading to the observed \(s_{ig}\).  Then the likelihood
functions \(h_{ig}\) and \(h'_{ig}\) for \(S_{ig}\) can be expressed in terms
of \(\{f_{ig}\}_{(i,g)\in\mathcal{S}}\):
\begin{eqnarray}
\label{eq:S-pmf-given-n}
h_{ig}(s_{ig} | n_{ig}, p_h) &=& \sum_{(i,g)\in\mathcal{S}} f_{ig}(y_{ig} | n_{ig}, p_h)
\\
\label{eq:S-pmf-given-dist-of-n}
h'_{ig}(s_{ig} | p_h) &=& \sum_{(i,g)\in\mathcal{S}} f_{ig}(y_{ig} | n_{ig},
p_h) \, q_{ig}(n_{ig}|p_h).
\end{eqnarray}
The difference between \(h_{ig}\) and \(h'_{ig}\) is whether or not we
condition the distribution of \(S_{ig}\) on the observed \(n_{ig}\).  If we
don't take advantage of the observations on \(n_{ig}\) (Eq.~\ref{eq:S-pmf-given-dist-of-n}), we
must then treat it as a random variable and specify a distribution for it, say
\(q_{ig}\). In either case we need \emph{some} kind of information or
assumption on
\(n_{ig}\).  This holds regardless we want to use \(h_{ig}\) (or \(h'_{ig}\))
in simulations, in parameter estimation or in classification with error
control.


\end{document}

\section{Basic model}

In my understanding, in Andy's general model
\(\{Y_{ig}\}_{ig}\) are independent random variables and
\begin{equation}
Y_{ig} \sim \mathrm{Binom}(q_h \text{ or } 1 - q_h, n_{ig}) \text{ under }
\mathcal{H}_h, \; h=0,1
\end{equation}

Let \(p_h = \mathrm{max}(q_h, 1-q_h)\).  In Andy's specific model \(p_0 =
1/2\) and \(p_1=9/10\).  To specify the model more completely, suppose \(p_h =
q_h\) with \(1/2\) probability \emph{a priori}.  Then for each \((i,g)\) the
probability mass function of \(Y_{ig}\)'s sampling distribution is
\begin{equation}
f(y|p_h, n_{ig}) = \frac{1}{2} \frac{n_{ig}!}{y! (n_{ig}-y)!} \left[ p_h^{y}
(1-p)^{n_{ig}-y} + p^{n_{ig}-y} (1-p)^{y} \right].
\end{equation}

Note that for homozygous \((i,g)\) pairs \(f(y=n_{ig}|p_h,n_{ig})=1\) for \(h=0,1\)
because all reads must surely come from a single variant regardless of allelic
exclusion.


For the observation \(Y_{ig}=y_{ig}\) the \(p\)-value is 
\begin{equation}
\sum_{y=y_{ig}}^{n_{ig}} f(y|p_0,n_{ig}).
\end{equation}

Set classification threshold \(n_{ig} t\) for any \(Y_{ig}\).  For instance,
\(t=0.9\) means that we classify those pairs \((i,g)\) for which at least
\(9/10\) of the reads come from the reference allele.  Let
\(\pi_0\) and \(\pi_1\) be the fraction of \((i,g)\) pairs when
\((i,g)\in\mathcal{H}_0\) and when
\((i,g)\in\mathcal{H}_1\), respectively.  Note that \(\pi_0+\pi_1=1\).

The expected number of \((i,g)\) pairs called monoallelic is then
\begin{equation}
\sum_{i,g} \pi_0 \overbrace{ \sum_{y=t}^{n_{ig}} f(y|p_0,n_{ig})}^{\text{false
positive rate}} + \pi_1
\overbrace{ \sum_{y=t}^{n_{ig}} f(y|p_1,n_{ig})}^{\text{true positive rate}}.
\end{equation}

So, given \(t\), there are two ways to learn about the expected number of positives
Define \(S_{ig}=n_{ig}^{-1} \mathrm{max}(Y_{ig}, n_{ig} - Y_{ig})\).  Then we
have
\begin{equation}
\label{eq:pmf-s-pmf-y}
f_s(s|p_h,n_{ig}) = f(y|p_h,n_{ig})
\end{equation}
as a consequence of the definition of \(p_h\), so it doesn't matter if we use
\(S_{ig}\) or \(Y_{ig}\) for testing \(H_h\) or inferring \(p_h\) as long as
we use the information \(n_{ig}\).  If may remove \(n_{ig}\) from
Eq.~\ref{eq:pmf-s-pmf-y} if we take it as a random quantity, specify a
distribution for it, and marginalize \(f_s\).  But then
\begin{equation}
\label{eq:pmf-smarginal-pmf-y}
f_s(s|p_h) \neq f(y|p_h,n_{ig})
\end{equation}
because we lost the information in the observed total number of reads
\(n_{ig}\).  This information loss prevents \(S_{ig}\) from being a sufficient
statistic.

